# SwapnilB_EcommerceSearchAnalytics_Assessment
E-Commerce Search Analytics Dashboard

# ğŸ“Š Node.js ETL Pipeline for Ecommerce Search Analytics (Supabase / PostgreSQL)

## Overview

This project implements a **Supabase-safe, production-ready Node.js ETL pipeline** designed to process large CSV files (800k+ rows) containing ecommerce search analytics data.

The pipeline emphasizes:
- Memory-efficient streaming
- Foreign-key safe inserts
- Stable long-running execution
- Compatibility with Supabase Session Mode

A **two-pass streaming architecture** ensures correct ordering between parent and child tables.

---

## âœ… Features

- Streaming CSV parsing (no full file load into memory)
- Foreign-key safe insert ordering
- Automatic schema creation
- Batch inserts with retry and exponential backoff
- Progress logging for long-running jobs
- Supabase session-safe (`pool.max = 1`)
- Clean Git hygiene (`raw_data` folder committed, CSV files ignored)

---

## ğŸ“ Project Structure

node-etl/
â”œâ”€â”€ index.js # Entry point
â”œâ”€â”€ load_fast_insert_pool_safe.js # Core ETL logic
â”œâ”€â”€ extract.js # Optional CSV downloader
â”œâ”€â”€ transform.js # Utility helpers
â”œâ”€â”€ raw_data/ # CSV input directory
â”‚ â””â”€â”€ .gitkeep
â”œâ”€â”€ .env # Environment variables (INCLUDED in git)
â”œâ”€â”€ package.json
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

---

## ğŸ”§ Prerequisites

Ensure the following are installed:

### 1ï¸âƒ£ Node.js
- Version **18 or higher**

```bash
node -v

2ï¸âƒ£ PostgreSQL / Supabase

A running PostgreSQL database or Supabase project
Supabase Session Mode enabled

3ï¸âƒ£ Git
git --version

ğŸ“¦ Dependencies

Installed via npm install:
1. pg â€“ PostgreSQL client
2. csv-parse â€“ Streaming CSV parsing
3. dotenv â€“ Environment variable loader
4. axios â€“ Optional CSV downloader

âš™ï¸ Environment Configuration
âœ… .env File (Tracked in Git)

Unlike most projects, this repository intentionally includes the .env file in version control.

âš ï¸ Important:
Before running the ETL, always cross-check the .env values to ensure they point to the correct database (local / staging / production).

Example .env file
PG_CONNECTION_STRING=postgres://USER:PASSWORD@HOST:PORT/postgres

ğŸ“‚ Raw Data Setup

Place CSV files inside the raw_data/ directory:

raw_data/
â””â”€â”€ nov 15 - nov 30.csv

Notes
Only .csv files are processed
CSV contents are ignored by Git
The raw_data folder itself is committed using .gitkeep

ğŸ—„ Database Schema

The ETL automatically creates all required tables if they do not exist.

Core Tables
1. customers
2. brands
3. categories
4. collections
5. searches

Child / Mapping Tables
1. search_metrics
2. ip_addresses
3. search_brands
4. search_categories
5. search_collections

All foreign keys are enforced and inserts are ordered to prevent FK violations.

ğŸ”„ ETL Execution Flow
Internal Processing Steps

1. First Pass (Streaming)
Reads CSV row-by-row
Collects lookup values
Writes rows to a temporary JSONL file

2. Lookup Inserts
Inserts customers, brands, categories, and collections
Builds in-memory ID maps

3. Second Pass â€“ Parent Inserts
Inserts all records into searches
Flushes remaining batches

4. Second Pass â€“ Child Inserts
Inserts metrics, IP addresses, and mapping tables
Maintains foreign-key safety

5. Cleanup
Removes temporary files
Closes database connections

â–¶ï¸ Running the ETL
1ï¸âƒ£ Install dependencies
npm install

2ï¸âƒ£ Verify environment
âœ… Cross-check .env values

3ï¸âƒ£ Run the pipeline
node index.js